<!DOCTYPE html>
<html>
    <head>
      <link rel="stylesheet" href="style.css">
      <script src="https://cdnjs.cloudflare.com/ajax/libs/onnxruntime-web/1.17.0/ort.webgpu.min.js"></script>
    </head>
    <body>
        <div class="container">
            <h1>Rapid Chat</h1>
            <h2>Talk to a LLAMA, see what it knows.</h2>
          
            <p class="comment">Current backend is onnx-web-runtime-wasm.</p>
          
            <div class="imessage">
            </div>
            <textarea id="input" name="input">Explain thermodynamics in simple terms.</textarea>
            <button id="send"></button>
        </div>
    </body>
    <script type="module" >
        import {AutoTokenizer} from 'https://cdn.jsdelivr.net/npm/@xenova/transformers';
        import {Sampler} from '/sampler.js';

        let tokenizer = null;
        let onnx_session = null;
        let sampler = null;

        // Text generation parameters.
        const max_tokens = 256;
        const temperature = 0.7;
        const topk = 50;
    
        async function init()
        {
            if (!tokenizer)
            {
              tokenizer = await AutoTokenizer.from_pretrained('tokenizer');
            }
            if (!onnx_session) {
                const onnx_model = 'https://huggingface.co/shoibl/TinyLlama-Chat-v1.1-onnx_quantized/resolve/main/onnx/decoder_model_merged_quantized.onnx?download=true';
                let cache = await caches.open("onnx")
                let model_response = await cache.match(onnx_model);
                if (!model_response)
                {
                    await cache.add(onnx_model);
                    model_response = await cache.match(onnx_model);
                }
                onnx_session = await ort.InferenceSession.create(await model_response.arrayBuffer(), {
                    quantized: true,
                    session_options: {
                        executionProviders: ["wasm"]
                    }
                });
            }
            if (!sampler)
            {
                sampler = Sampler.getSampler({do_sample: true, num_beams: 1, top_k: topk, temperature: temperature});
            }
        }

        // All the state that main sets up so that generateNextToken can keep pumping
        // tokens.
        let past_sequence_length = 0;
        let output_tokens = [];
        let model_inputs = null;
        let update_element = null;

        async function generateNextToken()
        {
            // Popluate position_ids.
            for (let pos = 0; pos < model_inputs['input_ids'].data.length; pos++) {
                model_inputs['position_ids'].data[pos]=BigInt(past_sequence_length + pos);
            }

            // Run inference
            const results = await onnx_session.run(model_inputs);

            let prev_input_length = model_inputs['input_ids'].dims.slice(-1)[0];
            let last_token = sampler.sample(results.logits, prev_input_length - 1);

            // LLama has spoken.
            if (last_token[0][0] == BigInt(2) || output_tokens.length > max_tokens) {
                // Clean up unused memory.
                past_sequence_length = 0;
                output_tokens = [];
                model_inputs = null;
                update_element = null;
                return;
            }

            output_tokens.push(last_token[0][0]);
            past_sequence_length += prev_input_length;

            // Prep for next iteration
            // New input is just the last token
            let new_input_ids = new BigInt64Array(1);
            new_input_ids[0] = BigInt(last_token[0][0]);
            model_inputs['input_ids'] = new ort.Tensor(new_input_ids, [1, 1]);
            let attention_mask =  new BigInt64Array(past_sequence_length+1);
            attention_mask.fill(BigInt(1), 0, past_sequence_length + 1);

            model_inputs['attention_mask'] = new ort.Tensor(attention_mask, [1, past_sequence_length+1]);
            model_inputs['position_ids'] = new ort.Tensor(new BigInt64Array(1), [1 , 1]);
            for (let ctx = 0; ctx < 22; ctx++)
            {
                let ctx_string = ctx.toString();
                model_inputs['past_key_values.'+ ctx_string +'.key'] = results['present.'+ctx_string+'.key'];
                model_inputs['past_key_values.'+ ctx_string +'.value'] = results['present.'+ctx_string+'.value'];
            }

            const output = await tokenizer.decode(output_tokens);
            update_element.innerText = output;
            window.requestAnimationFrame(generateNextToken);
        }

        async function main(input)
        {
            await init();
            const tokenizer_output = await tokenizer(input);
            let { input_ids, attention_mask } = tokenizer_output;
 
            past_sequence_length = 0;
            output_tokens = [];
            let past_key_values_size = 1 * 4 * past_sequence_length * 64;
            model_inputs = { 
                    input_ids: new ort.Tensor(input_ids.data, input_ids.dims),
                    attention_mask: new ort.Tensor(attention_mask.data, attention_mask.dims), 
                    position_ids: new ort.Tensor(new BigInt64Array(input_ids.data.length), [1,input_ids.data.length]), 
                    'past_key_values.0.key': new ort.Tensor(new Float32Array(past_key_values_size), [1,4,past_sequence_length,64]),
                    'past_key_values.0.value': new ort.Tensor(new Float32Array(past_key_values_size), [1,4,past_sequence_length,64]),
                    'past_key_values.1.key': new ort.Tensor(new Float32Array(past_key_values_size), [1,4,past_sequence_length,64]),
                    'past_key_values.1.value': new ort.Tensor(new Float32Array(past_key_values_size), [1,4,past_sequence_length,64]),
                    'past_key_values.2.key': new ort.Tensor(new Float32Array(past_key_values_size), [1,4,past_sequence_length,64]),
                    'past_key_values.2.value': new ort.Tensor(new Float32Array(past_key_values_size), [1,4,past_sequence_length,64]),
                    'past_key_values.3.key': new ort.Tensor(new Float32Array(past_key_values_size), [1,4,past_sequence_length,64]),
                    'past_key_values.3.value': new ort.Tensor(new Float32Array(past_key_values_size), [1,4,past_sequence_length,64]),
                    'past_key_values.4.key': new ort.Tensor(new Float32Array(past_key_values_size), [1,4,past_sequence_length,64]),
                    'past_key_values.4.value': new ort.Tensor(new Float32Array(past_key_values_size), [1,4,past_sequence_length,64]),
                    'past_key_values.5.key': new ort.Tensor(new Float32Array(past_key_values_size), [1,4,past_sequence_length,64]),
                    'past_key_values.5.value': new ort.Tensor(new Float32Array(past_key_values_size), [1,4,past_sequence_length,64]),
                    'past_key_values.6.key': new ort.Tensor(new Float32Array(past_key_values_size), [1,4,past_sequence_length,64]),
                    'past_key_values.6.value': new ort.Tensor(new Float32Array(past_key_values_size), [1,4,past_sequence_length,64]),
                    'past_key_values.7.key': new ort.Tensor(new Float32Array(past_key_values_size), [1,4,past_sequence_length,64]),
                    'past_key_values.7.value': new ort.Tensor(new Float32Array(past_key_values_size), [1,4,past_sequence_length,64]),
                    'past_key_values.8.key': new ort.Tensor(new Float32Array(past_key_values_size), [1,4,past_sequence_length,64]),
                    'past_key_values.8.value': new ort.Tensor(new Float32Array(past_key_values_size), [1,4,past_sequence_length,64]),
                    'past_key_values.9.key': new ort.Tensor(new Float32Array(past_key_values_size), [1,4,past_sequence_length,64]),
                    'past_key_values.9.value': new ort.Tensor(new Float32Array(past_key_values_size), [1,4,past_sequence_length,64]),
                    'past_key_values.10.key': new ort.Tensor(new Float32Array(past_key_values_size), [1,4,past_sequence_length,64]),
                    'past_key_values.10.value': new ort.Tensor(new Float32Array(past_key_values_size), [1,4,past_sequence_length,64]),
                    'past_key_values.11.key': new ort.Tensor(new Float32Array(past_key_values_size), [1,4,past_sequence_length,64]),
                    'past_key_values.11.value': new ort.Tensor(new Float32Array(past_key_values_size), [1,4,past_sequence_length,64]),
                    'past_key_values.12.key': new ort.Tensor(new Float32Array(past_key_values_size), [1,4,past_sequence_length,64]),
                    'past_key_values.12.value': new ort.Tensor(new Float32Array(past_key_values_size), [1,4,past_sequence_length,64]),
                    'past_key_values.13.key': new ort.Tensor(new Float32Array(past_key_values_size), [1,4,past_sequence_length,64]),
                    'past_key_values.13.value': new ort.Tensor(new Float32Array(past_key_values_size), [1,4,past_sequence_length,64]),
                    'past_key_values.14.key': new ort.Tensor(new Float32Array(past_key_values_size), [1,4,past_sequence_length,64]),
                    'past_key_values.14.value': new ort.Tensor(new Float32Array(past_key_values_size), [1,4,past_sequence_length,64]),
                    'past_key_values.15.key': new ort.Tensor(new Float32Array(past_key_values_size), [1,4,past_sequence_length,64]),
                    'past_key_values.15.value': new ort.Tensor(new Float32Array(past_key_values_size), [1,4,past_sequence_length,64]),
                    'past_key_values.16.key': new ort.Tensor(new Float32Array(past_key_values_size), [1,4,past_sequence_length,64]),
                    'past_key_values.16.value': new ort.Tensor(new Float32Array(past_key_values_size), [1,4,past_sequence_length,64]),
                    'past_key_values.17.key': new ort.Tensor(new Float32Array(past_key_values_size), [1,4,past_sequence_length,64]),
                    'past_key_values.17.value': new ort.Tensor(new Float32Array(past_key_values_size), [1,4,past_sequence_length,64]),
                    'past_key_values.18.key': new ort.Tensor(new Float32Array(past_key_values_size), [1,4,past_sequence_length,64]),
                    'past_key_values.18.value': new ort.Tensor(new Float32Array(past_key_values_size), [1,4,past_sequence_length,64]),
                    'past_key_values.19.key': new ort.Tensor(new Float32Array(past_key_values_size), [1,4,past_sequence_length,64]),
                    'past_key_values.19.value': new ort.Tensor(new Float32Array(past_key_values_size), [1,4,past_sequence_length,64]),
                    'past_key_values.20.key': new ort.Tensor(new Float32Array(past_key_values_size), [1,4,past_sequence_length,64]),
                    'past_key_values.20.value': new ort.Tensor(new Float32Array(past_key_values_size), [1,4,past_sequence_length,64]),
                    'past_key_values.21.key': new ort.Tensor(new Float32Array(past_key_values_size), [1,4,past_sequence_length,64]),
                    'past_key_values.21.value': new ort.Tensor(new Float32Array(past_key_values_size), [1,4,past_sequence_length,64]),
                };
      
            window.requestAnimationFrame(generateNextToken);
        }

        async function onSend() {
            const inputText = document.getElementById("input").value;
            const prompt = "<|system|>\nYou are a friendly assistant.</s>\n<|user|>"+inputText+"</s>\n<|assistant|>\n";
            let input = document.createElement("p");
            input.setAttribute("class", "from-me");
            input.innerText = inputText;
            document.getElementsByClassName("imessage")[0].appendChild(input);
            let reply = document.createElement("p");
            reply.setAttribute("class", "from-them");
            document.getElementsByClassName("imessage")[0].appendChild(reply);
            update_element = reply;
            reply.innerText = "...";
            main(prompt);
        }
        document.getElementById("send").onclick = onSend;
    </script>
</html>