<!DOCTYPE html>
<html>
    <head>
      <link rel="stylesheet" href="Style.css">
      <link rel="icon" type="image/x-icon" href="favicon.ico">
      <script src="https://cdnjs.cloudflare.com/ajax/libs/onnxruntime-web/1.17.0/ort.webgpu.min.js"></script>
      <script src="models/final_model_reordered.js"></script>
      <script src="models/CpuOps.js"></script>
    </head>
    <body>
        <div class="container">
            <h1>Rapid Chat</h1>
            <h2>Talk to a LLAMA, see what it knows.</h2>
          
            <p class="comment">Current backend is 
                <select name="backend" id="backend" class="select-dropdown">
                    <option value="RapidWebNN">RapidWebNN</option>
                    <option value="onnx-web-runtime-wasm">onnx-web-runtime-wasm</option>
                </select>
            </p>
          
            <div class="imessage">
            </div>
            <textarea id="input" name="input">Explain thermodynamics in simple terms.</textarea>
            <button id="send"></button>
        </div>
    </body>
    <script type="module" >
        import {AutoTokenizer} from 'https://cdn.jsdelivr.net/npm/@xenova/transformers';
        import {Sampler} from '/sampler.js';

        let tokenizer = null;
        let onnx_session = null;
        let webnn_weights = null;
        let sampler = null;

        // Text generation parameters.
        const max_tokens = 256;
        const temperature = 0.2;
        const topk = 10;
    
        async function fetchCached(url)
        {
            return fetch(url);
            let cache = await caches.open("RapidChat")
            let model_response = await cache.match(url);
            if (!model_response)
            {
                await cache.add(url);
                model_response = await cache.match(url);
            }
            return model_response;
        }
    
        async function init()
        {
            if (!tokenizer)
            {
              tokenizer = await AutoTokenizer.from_pretrained('/tokenizer');
            }
            if (document.getElementById("backend").value == "RapidWebNN")
            {
                if (!webnn_weights)
                {
                    webnn_weights = await fetchCached("/models/Weights.bin");
                    webnn_weights = await webnn_weights.arrayBuffer();
                }
            }
            else
            {
                if (!onnx_session) {
                    const onnx_model = 'https://huggingface.co/shoibl/TinyLlama-Chat-v1.1-onnx_quantized/resolve/main/onnx/decoder_model_merged_quantized.onnx?download=true';
                    let model_response = await fetchCached(onnx_model);
                    onnx_session = await ort.InferenceSession.create(await model_response.arrayBuffer(), {
                        quantized: true,
                        session_options: {
                            executionProviders: ["wasm"]
                        }
                    });
                }
            }
            if (!sampler)
            {
                sampler = Sampler.getSampler({do_sample: true, num_beams: 1, top_k: topk, temperature: temperature});
            }
        }

        // All the state that main sets up so that generateNextToken can keep pumping
        // tokens.
        let past_sequence_length = 0;
        let output_tokens = [];
        let model_inputs = null;
        let update_element = null;

        function getTensor(array, dims)
        {
            if (document.getElementById("backend").value == "RapidWebNN")
            {
                for (let index in dims)
                {
                    if (dims[index] == 0) {
                        return new NullTensor(dims);
                    }
                }
                if (array instanceof BigInt64Array)
                {
                    array.operand_desc = {type: 'int64', dataType: 'int64', dimensions: dims};
                }
                else if (array instanceof Float32Array)
                {
                    array.operand_desc = {type: 'float32', dataType: 'float32', dimensions: dims};
                }
                else
                {
                    throw new Exception("Unimplemented getTensor");
                }
                array.data = array;
                return array;
            }
            else
            {
                return new ort.Tensor(array, dims);
            }
        }
        async function generateNextToken()
        {
            // Popluate position_ids.
            for (let pos = 0; pos < model_inputs['input_ids'].data.length; pos++) {
                model_inputs['position_ids'].data[pos]=BigInt(past_sequence_length + pos);
            }

            // Run inference
            if (document.getElementById("backend").value == "RapidWebNN") {
                const context = await navigator.ml.createContext(
                    {'deviceType' : 'gpu'});
                let webnn_builder = new MLGraphBuilder(context);
                InstallCpuOps(webnn_builder);
                let graph_inputs=[];
                for (let key of Object.keys(model_inputs)){
                    if (model_inputs[key] instanceof NullTensor)
                    {
                        graph_inputs.push(model_inputs[key]);
                    }
                    else
                    {
                        graph_inputs.push(webnn_builder.input(key,  model_inputs[key].operand_desc));
                    }
                }
                graph_inputs.push(webnn_weights);
                graph_inputs.push(webnn_builder);
                const output_operands = loadModelGraph(...graph_inputs);
                const graph = await webnn_builder.build(output_operands);
                const outputs = {};
                let result = await context.compute(graph, model_inputs, outputs);
            } else {
                const results = await onnx_session.run(model_inputs);
            }

            let prev_input_length = model_inputs['input_ids'].dims.slice(-1)[0];
            let last_token = sampler.sample(results.logits, prev_input_length - 1);

            // LLama has spoken.
            if (last_token[0][0] == BigInt(2) || output_tokens.length > max_tokens) {
                // Clean up unused memory.
                past_sequence_length = 0;
                output_tokens = [];
                model_inputs = null;
                update_element = null;
                return;
            }

            output_tokens.push(last_token[0][0]);
            past_sequence_length += prev_input_length;

            // Prep for next iteration
            // New input is just the last token
            let new_input_ids = new BigInt64Array(1);
            new_input_ids[0] = BigInt(last_token[0][0]);
            model_inputs['input_ids'] = getTensor(new_input_ids, [1, 1]);
            let attention_mask =  new BigInt64Array(past_sequence_length+1);
            attention_mask.fill(BigInt(1), 0, past_sequence_length + 1);

            model_inputs['attention_mask'] = getTensor(attention_mask, [1, past_sequence_length+1]);
            model_inputs['position_ids'] = getTensor(new BigInt64Array(1), [1 , 1]);
            for (let ctx = 0; ctx < 22; ctx++)
            {
                let ctx_string = ctx.toString();
                model_inputs['past_key_values.'+ ctx_string +'.key'] = results['present.'+ctx_string+'.key'];
                model_inputs['past_key_values.'+ ctx_string +'.value'] = results['present.'+ctx_string+'.value'];
            }

            const output = await tokenizer.decode(output_tokens);
            update_element.innerText = output;
            update_element.scrollIntoView(false);
            window.requestAnimationFrame(generateNextToken);
        }

        async function main(input)
        {
            await init();
            const tokenizer_output = await tokenizer(input);
            let { input_ids, attention_mask } = tokenizer_output;
 
            past_sequence_length = 0;
            output_tokens = [];
            let past_key_values_size = 1 * 4 * past_sequence_length * 64;
            model_inputs = { 
                    input_ids: getTensor(input_ids.data, input_ids.dims),
                    attention_mask: getTensor(attention_mask.data, attention_mask.dims), 
                    position_ids: getTensor(new BigInt64Array(input_ids.data.length), [1,input_ids.data.length]), 
                    'past_key_values.0.key': getTensor(new Float32Array(past_key_values_size), [1,4,past_sequence_length,64]),
                    'past_key_values.0.value': getTensor(new Float32Array(past_key_values_size), [1,4,past_sequence_length,64]),
                    'past_key_values.1.key': getTensor(new Float32Array(past_key_values_size), [1,4,past_sequence_length,64]),
                    'past_key_values.1.value': getTensor(new Float32Array(past_key_values_size), [1,4,past_sequence_length,64]),
                    'past_key_values.2.key': getTensor(new Float32Array(past_key_values_size), [1,4,past_sequence_length,64]),
                    'past_key_values.2.value': getTensor(new Float32Array(past_key_values_size), [1,4,past_sequence_length,64]),
                    'past_key_values.3.key': getTensor(new Float32Array(past_key_values_size), [1,4,past_sequence_length,64]),
                    'past_key_values.3.value': getTensor(new Float32Array(past_key_values_size), [1,4,past_sequence_length,64]),
                    'past_key_values.4.key': getTensor(new Float32Array(past_key_values_size), [1,4,past_sequence_length,64]),
                    'past_key_values.4.value': getTensor(new Float32Array(past_key_values_size), [1,4,past_sequence_length,64]),
                    'past_key_values.5.key': getTensor(new Float32Array(past_key_values_size), [1,4,past_sequence_length,64]),
                    'past_key_values.5.value': getTensor(new Float32Array(past_key_values_size), [1,4,past_sequence_length,64]),
                    'past_key_values.6.key': getTensor(new Float32Array(past_key_values_size), [1,4,past_sequence_length,64]),
                    'past_key_values.6.value': getTensor(new Float32Array(past_key_values_size), [1,4,past_sequence_length,64]),
                    'past_key_values.7.key': getTensor(new Float32Array(past_key_values_size), [1,4,past_sequence_length,64]),
                    'past_key_values.7.value': getTensor(new Float32Array(past_key_values_size), [1,4,past_sequence_length,64]),
                    'past_key_values.8.key': getTensor(new Float32Array(past_key_values_size), [1,4,past_sequence_length,64]),
                    'past_key_values.8.value': getTensor(new Float32Array(past_key_values_size), [1,4,past_sequence_length,64]),
                    'past_key_values.9.key': getTensor(new Float32Array(past_key_values_size), [1,4,past_sequence_length,64]),
                    'past_key_values.9.value': getTensor(new Float32Array(past_key_values_size), [1,4,past_sequence_length,64]),
                    'past_key_values.10.key': getTensor(new Float32Array(past_key_values_size), [1,4,past_sequence_length,64]),
                    'past_key_values.10.value': getTensor(new Float32Array(past_key_values_size), [1,4,past_sequence_length,64]),
                    'past_key_values.11.key': getTensor(new Float32Array(past_key_values_size), [1,4,past_sequence_length,64]),
                    'past_key_values.11.value': getTensor(new Float32Array(past_key_values_size), [1,4,past_sequence_length,64]),
                    'past_key_values.12.key': getTensor(new Float32Array(past_key_values_size), [1,4,past_sequence_length,64]),
                    'past_key_values.12.value': getTensor(new Float32Array(past_key_values_size), [1,4,past_sequence_length,64]),
                    'past_key_values.13.key': getTensor(new Float32Array(past_key_values_size), [1,4,past_sequence_length,64]),
                    'past_key_values.13.value': getTensor(new Float32Array(past_key_values_size), [1,4,past_sequence_length,64]),
                    'past_key_values.14.key': getTensor(new Float32Array(past_key_values_size), [1,4,past_sequence_length,64]),
                    'past_key_values.14.value': getTensor(new Float32Array(past_key_values_size), [1,4,past_sequence_length,64]),
                    'past_key_values.15.key': getTensor(new Float32Array(past_key_values_size), [1,4,past_sequence_length,64]),
                    'past_key_values.15.value': getTensor(new Float32Array(past_key_values_size), [1,4,past_sequence_length,64]),
                    'past_key_values.16.key': getTensor(new Float32Array(past_key_values_size), [1,4,past_sequence_length,64]),
                    'past_key_values.16.value': getTensor(new Float32Array(past_key_values_size), [1,4,past_sequence_length,64]),
                    'past_key_values.17.key': getTensor(new Float32Array(past_key_values_size), [1,4,past_sequence_length,64]),
                    'past_key_values.17.value': getTensor(new Float32Array(past_key_values_size), [1,4,past_sequence_length,64]),
                    'past_key_values.18.key': getTensor(new Float32Array(past_key_values_size), [1,4,past_sequence_length,64]),
                    'past_key_values.18.value': getTensor(new Float32Array(past_key_values_size), [1,4,past_sequence_length,64]),
                    'past_key_values.19.key': getTensor(new Float32Array(past_key_values_size), [1,4,past_sequence_length,64]),
                    'past_key_values.19.value': getTensor(new Float32Array(past_key_values_size), [1,4,past_sequence_length,64]),
                    'past_key_values.20.key': getTensor(new Float32Array(past_key_values_size), [1,4,past_sequence_length,64]),
                    'past_key_values.20.value': getTensor(new Float32Array(past_key_values_size), [1,4,past_sequence_length,64]),
                    'past_key_values.21.key': getTensor(new Float32Array(past_key_values_size), [1,4,past_sequence_length,64]),
                    'past_key_values.21.value': getTensor(new Float32Array(past_key_values_size), [1,4,past_sequence_length,64]),
                };
      
            window.requestAnimationFrame(generateNextToken);
        }

        async function onSend() {
            const inputText = document.getElementById("input").value;
            const prompt = "<|system|>: You are a friendly asssitant, that should respond to user in a short and concise way. </s>\n<|user|>:"+inputText+"</s>\n<|assistant|>:\n";
            let input = document.createElement("p");
            input.setAttribute("class", "from-me");
            input.innerText = inputText;
            document.getElementsByClassName("imessage")[0].appendChild(input);
            let reply = document.createElement("p");
            reply.setAttribute("class", "from-them");
            document.getElementsByClassName("imessage")[0].appendChild(reply);
            update_element = reply;
            reply.innerText = "...";
            main(prompt);
        }
        document.getElementById("send").onclick = onSend;
    </script>
</html>