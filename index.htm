<!DOCTYPE html>
<html>
    <head>
      <link rel="stylesheet" href="Style.css">
      <link rel="icon" type="image/x-icon" href="favicon.ico">
      <script src="https://cdnjs.cloudflare.com/ajax/libs/onnxruntime-web/1.17.0/ort.webgpu.min.js"></script>
      <script src="models/final_model_reordered.js"></script>
      <script src="models/CpuOps.js"></script>
    </head>
    <body>
        <div class="container">
            <h1>Rapid Chat</h1>
            <h2>Talk to a LLAMA, see what it knows.</h2>
          
            <p class="comment">Current backend is 
                <select name="backend" id="backend" class="select-dropdown">
                    <option value="RapidWebNN">RapidWebNN</option>
                    <option value="onnx-web-runtime-wasm">onnx-web-runtime-wasm</option>
                </select>
            </p>
          
            <div class="imessage">
            </div>
            <textarea id="input" name="input">Explain thermodynamics in simple terms.</textarea>
            <button id="send"></button>
        </div>
    </body>
    <script type="module" >
        import {AutoTokenizer} from 'https://cdn.jsdelivr.net/npm/@xenova/transformers';
        import {Sampler} from './sampler.js';

        let tokenizer = null;
        let onnx_session = null;
        let webnn_weights = null;
        let sampler = null;

        // Text generation parameters.
        const max_tokens = 256;
        const temperature = 0.2;
        const topk = 10;
        window.webnn_optimize = false;
    
        async function fetchCached(url)
        {
            let cache = await caches.open("RapidChat")
            let model_response = await cache.match(url);
            if (!model_response)
            {
                await cache.add(url);
                model_response = await cache.match(url);
            }
            return model_response;
        }
    
        async function init()
        {
            if (!tokenizer)
            {
              tokenizer = await AutoTokenizer.from_pretrained("../RapidChat/models/tokenizer");
            }
            if (document.getElementById("backend").value == "RapidWebNN")
            {
                if (!webnn_weights)
                {
                    webnn_weights = [];
                    for (let i = 0; i < 17; i++) {
                        // https://huggingface.co/sushanthr/tinyLlama-1.1B-Chat-v1.0-fp16-webnn/resolve/main/Weights0.bin?download=true
                        let weight = await (await fetchCached(`https://huggingface.co/sushanthr/tinyLlama-1.1B-Chat-v1.0-fp16-webnn/resolve/main/Weights${i}.bin?download=true`)).arrayBuffer();
                        webnn_weights.push(weight);
                    }                    
                }
            }
            else
            {
                if (!onnx_session) {
                    const onnx_model = 'https://huggingface.co/shoibl/TinyLlama-Chat-v1.1-onnx_quantized/resolve/main/onnx/decoder_model_merged_quantized.onnx?download=true';
                    let model_response = await fetchCached(onnx_model);
                    onnx_session = await ort.InferenceSession.create(await model_response.arrayBuffer(), {
                        quantized: true,
                        session_options: {
                            executionProviders: ["wasm"]
                        }
                    });
                }
            }
            if (!sampler)
            {
                sampler = Sampler.getSampler({do_sample: true, num_beams: 1, top_k: topk, temperature: temperature});
            }
        }

        // All the state that main sets up so that generateNextToken can keep pumping
        // tokens.
        let past_sequence_length = 0;
        let output_tokens = [];
        let model_inputs = null;
        let update_element = null;

        function getTensor(array, dims)
        {
            if (document.getElementById("backend").value == "RapidWebNN")
            {
                for (let index in dims)
                {
                    if (dims[index] == 0) {
                        return new NullTensor(dims);
                    }
                }
                if (array instanceof BigInt64Array)
                {
                    array.operand_desc = {type: 'int64', dataType: 'int64', dimensions: dims};
                }
                else if (array instanceof Float32Array)
                {
                    array.operand_desc = {type: 'float32', dataType: 'float32', dimensions: dims};
                }
                else if (array instanceof Uint16Array)
                {
                    array.operand_desc = {type: 'float16', dataType: 'float16', dimensions: dims};
                }
                else
                {
                    throw new Exception("Unimplemented getTensor");
                }
                array.data = array;
                return array;
            }
            else
            {
                return new ort.Tensor(array, dims);
            }
        }
        
        function mulReduce(arr) {
            return arr.reduce((product, value) => product * value, 1);
        }
        function allocateWebNNOutputBuffers(output_operands)
        {
            let output_buffers = {};
            for (let element in output_operands) {
                if (output_operands[element].dataType() == 'float32')
                {
                    output_buffers[element] = new Float32Array(mulReduce(output_operands[element].shape()));
                }
                if (output_operands[element].dataType() == 'float16') {
                    output_buffers[element] = new Uint16Array(mulReduce(output_operands[element].shape()));
                }
            }
            return output_buffers;
        }
        function replaceNullTensors(operands)
        {
            let result = {};
            for (let element in operands) {
                if (!(operands[element] instanceof NullTensor))
                {
                    result[element] = operands[element];
                }
            }
            return result;
        }
        let last_graph = null;
        let last_outputs = null;
        let last_input_size = 0;
        let last_output_operands = null;
        let last_context = null;
        async function generateNextToken()
        {
            // Popluate position_ids.
            for (let pos = 0; pos < model_inputs['input_ids'].data.length; pos++) {
                model_inputs['position_ids'].data[pos]=BigInt(past_sequence_length + pos);
            }
            let prev_input_length = model_inputs['input_ids'].data.length;
            let results = null;

            // Run inference
            let webnn_mode = document.getElementById("backend").value == "RapidWebNN";
            if (webnn_mode) {

                // Graph building
                if (!window.webnn_optimize || last_input_size != model_inputs.input_ids.operand_desc.dimensions[1])
                {
                    last_context = await navigator.ml.createContext({'deviceType' : 'gpu'}); 
                    let webnn_builder = new MLGraphBuilder(last_context);
                    InstallCpuOps(webnn_builder);
                    let graph_inputs=[];
                    for (let key of Object.keys(model_inputs)){
                        if (model_inputs[key] instanceof NullTensor)
                        {
                            graph_inputs.push(model_inputs[key]);
                        }
                        else
                        {
                            graph_inputs.push(webnn_builder.input(key,  model_inputs[key].operand_desc));
                        }
                    }
                    graph_inputs.push(webnn_weights);
                    graph_inputs.push(webnn_builder);
                    last_output_operands = loadModelGraph(...graph_inputs);
                    last_graph = await webnn_builder.build(last_output_operands);
                    last_input_size = model_inputs.input_ids.operand_desc.dimensions[1];
                }

                last_outputs = allocateWebNNOutputBuffers(last_output_operands);
                let webnn_inputs = replaceNullTensors(model_inputs);
                results = await last_context.compute(last_graph, webnn_inputs, last_outputs);
                results = results.outputs;
                results.logits = new ort.Tensor(results.logits, last_output_operands.logits.shape());
            } else {
                results = await onnx_session.run(model_inputs);
            }

            let last_token = sampler.sample(results.logits, prev_input_length - 1);
            // LLama has spoken.
            if (last_token[0][0] == BigInt(2) || output_tokens.length > max_tokens) {
                // Clean up unused memory.
                past_sequence_length = 0;
                output_tokens = [];
                model_inputs = null;
                update_element = null;
                return;
            }

            output_tokens.push(last_token[0][0]);
            past_sequence_length += prev_input_length;

            // Prep for next iteration
            // New input is just the last token
            if (webnn_mode && window.webnn_optimize)
            {
                let new_input_ids = new BigInt64Array(1);
                new_input_ids[0] = BigInt(last_token[0][0]);
                model_inputs['input_ids'] = getTensor(new_input_ids, [1, 1]);
                model_inputs['position_ids'] = getTensor(new BigInt64Array(1), [1 , 1]);

                let attention_mask =  new BigInt64Array(max_tokens);
                attention_mask.fill(BigInt(1), 0, past_sequence_length + 1);
                model_inputs['attention_mask'] = getTensor(attention_mask, [1, max_tokens]);
                let src_token_dim = last_output_operands['present.0.key'].shape()[2];
                for (let ctx = 0; ctx < 22; ctx++)
                {
                    let ctx_string = ctx.toString();
                    let count = 4 * (max_tokens - 1) * 64;
                    // for index, instead of length using count - 1 below.
                    let new_keys = new Uint16Array(count);
                    let new_values =  new Uint16Array(count);
                    let present_keys = results['present.'+ctx_string+'.key'];
                    let present_values = results['present.'+ctx_string+'.value'];
                    let last_kv_location = results['present.'+ctx_string+'.key'].length < new_keys.length ?  past_sequence_length - 1: max_tokens-1;
  
                    // We now have new kv cache values at the end that we need to copy
                    // to the correct spot in the new_keys.
                    for (let i = 0; i < 4; i++)
                    {
                        for (let j = 0; j < past_sequence_length-1; j++)
                        {
                            for (let k = 0; k < 64; k++)
                            {
                                let dst = i * (max_tokens - 1) * 64 + j * 64 + k;
                                let src = i *  (src_token_dim) * 64 + j * 64 + k;
                                new_keys[dst] = present_keys[src];
                                new_values[dst] = present_values[src];
                            }
                        }
                    }
                    // Now copy the new KV cache values.
                    for (let i = 0; i < 4; i++)
                    {
                        for (let k = 0; k < 64; k++)
                        {
                            let dst = i * (max_tokens - 1) * 64 + (past_sequence_length-1) * 64 + k;
                            let src = i *  (src_token_dim) * 64 + last_kv_location * 64 + k;
                            new_keys[dst] = present_keys[src];
                            new_values[dst] = present_values[src];
                        }
                    }
  
                    model_inputs['past_key_values.'+ ctx_string +'.key'] = getTensor(new_keys, [1, 4, max_tokens - 1, 64]);
                    model_inputs['past_key_values.'+ ctx_string +'.value'] = getTensor(new_values, [1, 4, max_tokens - 1, 64]); 
                }
            }
            else
            {
                let new_input_ids = new BigInt64Array(1);
                new_input_ids[0] = BigInt(last_token[0][0]);
                model_inputs['input_ids'] = getTensor(new_input_ids, [1, 1]);
                let attention_mask =  new BigInt64Array(past_sequence_length+1);
                attention_mask.fill(BigInt(1), 0, past_sequence_length + 1);
                model_inputs['attention_mask'] = getTensor(attention_mask, [1, past_sequence_length+1]);
                model_inputs['position_ids'] = getTensor(new BigInt64Array(1), [1 , 1]);
                for (let ctx = 0; ctx < 22; ctx++)
                {
                    let ctx_string = ctx.toString();
                    if (webnn_mode)
                    {
                        model_inputs['past_key_values.'+ ctx_string +'.key'] = getTensor(results['present.'+ctx_string+'.key'],last_output_operands[['present.'+ctx_string+'.key']].shape());
                        model_inputs['past_key_values.'+ ctx_string +'.value'] = getTensor(results['present.'+ctx_string+'.value'],last_output_operands[['present.'+ctx_string+'.value']].shape());
                    }
                    else 
                    {
                        model_inputs['past_key_values.'+ ctx_string +'.key'] = results['present.'+ctx_string+'.key'];
                        model_inputs['past_key_values.'+ ctx_string +'.value'] = results['present.'+ctx_string+'.value'];
                    }
                }
            }

            const output = await tokenizer.decode(output_tokens);
            update_element.innerText = output;
            update_element.scrollIntoView(false);
            window.requestAnimationFrame(generateNextToken);
        }

        async function main(input)
        {
            await init();
            const tokenizer_output = await tokenizer(input);
            let { input_ids, attention_mask } = tokenizer_output;
 
            past_sequence_length = 0;
            output_tokens = [];
            let past_key_values_size = 1 * 4 * past_sequence_length * 64;
            const num_heads = 22; // Number of layers (0 to 21)
            const webnn = (document.getElementById("backend").value == "RapidWebNN"); // Set this to true for Float32Array, false for Uint16Array
            model_inputs = {
                input_ids: getTensor(input_ids.data, input_ids.dims),
                attention_mask: getTensor(attention_mask.data, attention_mask.dims),
                position_ids: getTensor(new BigInt64Array(input_ids.data.length), [1, input_ids.data.length]),
            };
            for (let i = 0; i < num_heads; i++) {
                const ArrayType = webnn ? Uint16Array: Float32Array;
                model_inputs[`past_key_values.${i}.key`] = getTensor(
                    new ArrayType(past_key_values_size),
                    [1, 4, past_sequence_length, 64]
                );
                model_inputs[`past_key_values.${i}.value`] = getTensor(
                    new ArrayType(past_key_values_size),
                    [1, 4, past_sequence_length, 64]
                );
            }
            window.requestAnimationFrame(generateNextToken);
        }

        async function onSend() {
            const inputText = document.getElementById("input").value;
            const prompt = "<|system|>: You are a friendly asssitant, that should respond to user in a short and concise way. </s>\n<|user|>:"+inputText+"</s>\n<|assistant|>:\n";
            let input = document.createElement("p");
            input.setAttribute("class", "from-me");
            input.innerText = inputText;
            document.getElementsByClassName("imessage")[0].appendChild(input);
            let reply = document.createElement("p");
            reply.setAttribute("class", "from-them");
            document.getElementsByClassName("imessage")[0].appendChild(reply);
            update_element = reply;
            reply.innerText = "...";
            main(prompt);
        }
        document.getElementById("send").onclick = onSend;
    </script>
</html>