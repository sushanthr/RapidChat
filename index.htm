<!DOCTYPE html>
<html>
    <head>
      <link rel="stylesheet" href="Style.css">
      <link rel="icon" type="image/x-icon" href="favicon.ico">
      <script src="https://cdnjs.cloudflare.com/ajax/libs/onnxruntime-web/1.17.0/ort.webgpu.min.js"></script>
      <script src="models/final_model_reordered.js"></script>
      <script src="models/CpuOps.js"></script>
    </head>
    <body>
        <div class="container">
            <h1>Rapid Chat</h1>
            <h2>Talk to a LLAMA, see what it knows.</h2>
          
            <p class="comment">Current backend is 
                <select name="backend" id="backend" class="select-dropdown">
                    <option value="RapidWebNN">RapidWebNN</option>
                    <option value="onnx-web-runtime-wasm">onnx-web-runtime-wasm</option>
                </select>
            </p>
          
            <div class="imessage">
            </div>
            <textarea id="input" name="input">Explain thermodynamics in simple terms.</textarea>
            <button id="send"></button>
        </div>
    </body>
    <script type="module" >
        import {AutoTokenizer} from 'https://cdn.jsdelivr.net/npm/@xenova/transformers';
        import {Sampler} from './sampler.js';

        let tokenizer = null;
        let onnx_session = null;
        let webnn_weights = null;
        let sampler = null;

        // Text generation parameters.
        const max_tokens = 256;
        const temperature = 0.2;
        const topk = 10;
    
        async function fetchCached(url)
        {
            let cache = await caches.open("RapidChat")
            let model_response = await cache.match(url);
            if (!model_response)
            {
                await cache.add(url);
                model_response = await cache.match(url);
            }
            return model_response;
        }
    
        async function init()
        {
            if (!tokenizer)
            {
              tokenizer = await AutoTokenizer.from_pretrained('/tokenizer');
            }
            if (document.getElementById("backend").value == "RapidWebNN")
            {
                if (!webnn_weights)
                {
                    webnn_weights = [
                        await (await fetchCached("/models/Weights0.bin")).arrayBuffer(),
                        await (await fetchCached("/models/Weights1.bin")).arrayBuffer(),
                        await (await fetchCached("/models/Weights2.bin")).arrayBuffer(),
                    ];
                }
            }
            else
            {
                if (!onnx_session) {
                    const onnx_model = 'https://huggingface.co/shoibl/TinyLlama-Chat-v1.1-onnx_quantized/resolve/main/onnx/decoder_model_merged_quantized.onnx?download=true';
                    let model_response = await fetchCached(onnx_model);
                    onnx_session = await ort.InferenceSession.create(await model_response.arrayBuffer(), {
                        quantized: true,
                        session_options: {
                            executionProviders: ["wasm"]
                        }
                    });
                }
            }
            if (!sampler)
            {
                sampler = Sampler.getSampler({do_sample: true, num_beams: 1, top_k: topk, temperature: temperature});
            }
        }

        // All the state that main sets up so that generateNextToken can keep pumping
        // tokens.
        let past_sequence_length = 0;
        let output_tokens = [];
        let model_inputs = null;
        let update_element = null;

        function getTensor(array, dims)
        {
            if (document.getElementById("backend").value == "RapidWebNN")
            {
                for (let index in dims)
                {
                    if (dims[index] == 0) {
                        return new NullTensor(dims);
                    }
                }
                if (array instanceof BigInt64Array)
                {
                    array.operand_desc = {type: 'int64', dataType: 'int64', dimensions: dims};
                }
                else if (array instanceof Float32Array)
                {
                    array.operand_desc = {type: 'float32', dataType: 'float32', dimensions: dims};
                }
                else if (array instanceof Uint16Array)
                {
                    array.operand_desc = {type: 'float16', dataType: 'float16', dimensions: dims};
                }
                else
                {
                    throw new Exception("Unimplemented getTensor");
                }
                array.data = array;
                return array;
            }
            else
            {
                return new ort.Tensor(array, dims);
            }
        }
        
        function mulReduce(arr) {
            return arr.reduce((product, value) => product * value, 1);
        }
        function allocateWebNNOutputBuffers(output_operands)
        {
            let output_buffers = {};
            for (let element in output_operands) {
                if (output_operands[element].dataType() == 'float32')
                {
                    output_buffers[element] = new Float32Array(mulReduce(output_operands[element].shape()));
                }
                if (output_operands[element].dataType() == 'float16') {
                    output_buffers[element] = new Uint16Array(mulReduce(output_operands[element].shape()));
                }
            }
            return output_buffers;
        }
        function replaceNullTensors(operands)
        {
            let result = {};
            for (let element in operands) {
                if (!(operands[element] instanceof NullTensor))
                {
                    result[element] = operands[element];
                }
            }
            return result;
        }
        async function generateNextToken()
        {
            // Popluate position_ids.
            for (let pos = 0; pos < model_inputs['input_ids'].data.length; pos++) {
                model_inputs['position_ids'].data[pos]=BigInt(past_sequence_length + pos);
            }
            let prev_input_length = model_inputs['input_ids'].data.length;
            let results = null;
            let output_operands = null;

            // Run inference
            let webnn_mode = document.getElementById("backend").value == "RapidWebNN";
            if (webnn_mode) {
                const context = await navigator.ml.createContext(
                    {'deviceType' : 'gpu'});
                let webnn_builder = new MLGraphBuilder(context);
                InstallCpuOps(webnn_builder);
                let graph_inputs=[];
                for (let key of Object.keys(model_inputs)){
                    if (model_inputs[key] instanceof NullTensor)
                    {
                        graph_inputs.push(model_inputs[key]);
                    }
                    else
                    {
                        graph_inputs.push(webnn_builder.input(key,  model_inputs[key].operand_desc));
                    }
                }
                graph_inputs.push(webnn_weights);
                graph_inputs.push(webnn_builder);
                output_operands = loadModelGraph(...graph_inputs);
                const graph = await webnn_builder.build(output_operands);
                const outputs = allocateWebNNOutputBuffers(output_operands);
                let webnn_inputs = replaceNullTensors(model_inputs);
                results = await context.compute(graph, webnn_inputs, outputs);
                results = results.outputs;
                results.logits = new ort.Tensor(results.logits, output_operands.logits.shape());
            } else {
                results = await onnx_session.run(model_inputs);
            }

            let last_token = sampler.sample(results.logits, prev_input_length - 1);
            // LLama has spoken.
            if (last_token[0][0] == BigInt(2) || output_tokens.length > max_tokens) {
                // Clean up unused memory.
                past_sequence_length = 0;
                output_tokens = [];
                model_inputs = null;
                update_element = null;
                return;
            }

            output_tokens.push(last_token[0][0]);
            past_sequence_length += prev_input_length;

            // Prep for next iteration
            // New input is just the last token
            let new_input_ids = new BigInt64Array(1);
            new_input_ids[0] = BigInt(last_token[0][0]);
            model_inputs['input_ids'] = getTensor(new_input_ids, [1, 1]);
            let attention_mask =  new BigInt64Array(past_sequence_length+1);
            attention_mask.fill(BigInt(1), 0, past_sequence_length + 1);

            model_inputs['attention_mask'] = getTensor(attention_mask, [1, past_sequence_length+1]);
            model_inputs['position_ids'] = getTensor(new BigInt64Array(1), [1 , 1]);
            for (let ctx = 0; ctx < 22; ctx++)
            {
                let ctx_string = ctx.toString();
                if (webnn_mode)
                {
                    model_inputs['past_key_values.'+ ctx_string +'.key'] = getTensor(results['present.'+ctx_string+'.key'],output_operands[['present.'+ctx_string+'.key']].shape());
                    model_inputs['past_key_values.'+ ctx_string +'.value'] = getTensor(results['present.'+ctx_string+'.value'], output_operands[['present.'+ctx_string+'.value']].shape());
                }
                else 
                {
                    model_inputs['past_key_values.'+ ctx_string +'.key'] = results['present.'+ctx_string+'.key'];
                    model_inputs['past_key_values.'+ ctx_string +'.value'] = results['present.'+ctx_string+'.value'];
                }
            }

            const output = await tokenizer.decode(output_tokens);
            update_element.innerText = output;
            update_element.scrollIntoView(false);
            window.requestAnimationFrame(generateNextToken);
        }

        async function main(input)
        {
            await init();
            const tokenizer_output = await tokenizer(input);
            let { input_ids, attention_mask } = tokenizer_output;
 
            past_sequence_length = 0;
            output_tokens = [];
            let past_key_values_size = 1 * 4 * past_sequence_length * 64;
            const num_heads = 22; // Number of layers (0 to 21)
            const webnn = (document.getElementById("backend").value == "RapidWebNN"); // Set this to true for Float32Array, false for Uint16Array
            model_inputs = {
                input_ids: getTensor(input_ids.data, input_ids.dims),
                attention_mask: getTensor(attention_mask.data, attention_mask.dims),
                position_ids: getTensor(new BigInt64Array(input_ids.data.length), [1, input_ids.data.length]),
            };
            for (let i = 0; i < num_heads; i++) {
                const ArrayType = webnn ? Uint16Array: Float32Array;
                model_inputs[`past_key_values.${i}.key`] = getTensor(
                    new ArrayType(past_key_values_size),
                    [1, 4, past_sequence_length, 64]
                );
                model_inputs[`past_key_values.${i}.value`] = getTensor(
                    new ArrayType(past_key_values_size),
                    [1, 4, past_sequence_length, 64]
                );
            }
            window.requestAnimationFrame(generateNextToken);
        }

        async function onSend() {
            const inputText = document.getElementById("input").value;
            const prompt = "<|system|>: You are a friendly asssitant, that should respond to user in a short and concise way. </s>\n<|user|>:"+inputText+"</s>\n<|assistant|>:\n";
            let input = document.createElement("p");
            input.setAttribute("class", "from-me");
            input.innerText = inputText;
            document.getElementsByClassName("imessage")[0].appendChild(input);
            let reply = document.createElement("p");
            reply.setAttribute("class", "from-them");
            document.getElementsByClassName("imessage")[0].appendChild(reply);
            update_element = reply;
            reply.innerText = "...";
            main(prompt);
        }
        document.getElementById("send").onclick = onSend;
    </script>
</html>